{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-resolution models\n",
    "\n",
    "### EDSR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Add, Conv2D, Input, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "DIV2K_RGB_MEAN = np.array([0.4488, 0.4371, 0.4040]) * 255\n",
    "\n",
    "\n",
    "def edsr(scale, num_filters=64, num_res_blocks=8, res_block_scaling=None):\n",
    "    \"\"\"Creates an EDSR model.\"\"\"\n",
    "    x_in = Input(shape=(None, None, 3))\n",
    "    x = Lambda(normalize)(x_in)\n",
    "\n",
    "    x = b = Conv2D(num_filters, 3, padding='same')(x)\n",
    "    for i in range(num_res_blocks):\n",
    "        b = res_block(b, num_filters, res_block_scaling)\n",
    "    b = Conv2D(num_filters, 3, padding='same')(b)\n",
    "    x = Add()([x, b])\n",
    "\n",
    "    x = upsample(x, scale, num_filters)\n",
    "    x = Conv2D(3, 3, padding='same')(x)\n",
    "\n",
    "    x = Lambda(denormalize)(x)\n",
    "    return Model(x_in, x, name=\"edsr\")\n",
    "\n",
    "\n",
    "def res_block(x_in, filters, scaling):\n",
    "    \"\"\"Creates an EDSR residual block.\"\"\"\n",
    "    x = Conv2D(filters, 3, padding='same', activation='relu')(x_in)\n",
    "    x = Conv2D(filters, 3, padding='same')(x)\n",
    "    if scaling:\n",
    "        x = Lambda(lambda t: t * scaling)(x)\n",
    "    x = Add()([x_in, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(x, scale, num_filters):\n",
    "    def upsample_1(x, factor, **kwargs):\n",
    "        \"\"\"Sub-pixel convolution.\"\"\"\n",
    "        x = Conv2D(num_filters * (factor ** 2), 3, padding='same', **kwargs)(x)\n",
    "        return Lambda(pixel_shuffle(scale=factor))(x)\n",
    "\n",
    "    if scale == 2:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "    elif scale == 3:\n",
    "        x = upsample_1(x, 3, name='conv2d_1_scale_3')\n",
    "    elif scale == 4:\n",
    "        x = upsample_1(x, 2, name='conv2d_1_scale_2')\n",
    "        x = upsample_1(x, 2, name='conv2d_2_scale_2')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def pixel_shuffle(scale):\n",
    "    return lambda x: tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - DIV2K_RGB_MEAN) / 127.5\n",
    "\n",
    "\n",
    "def denormalize(x):\n",
    "    return x * 127.5 + DIV2K_RGB_MEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WDSR\n",
    "\n",
    "## Model training\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import DIV2K\n",
    "\n",
    "train = DIV2K(scale=4, downgrade='bicubic', subset='train')\n",
    "train_ds = train.dataset(batch_size=16, random_transform=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 4s 434ms/step - loss: 70.6978\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 4s 419ms/step - loss: 38.3402\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 28.7920\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "\n",
    "# Create directory for saving model weights\n",
    "weights_dir = 'weights/article'\n",
    "os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "# EDSR baseline as described in the EDSR paper (1.52M parameters)\n",
    "model_edsr = edsr(scale=4, num_res_blocks=16)\n",
    "\n",
    "# Adam optimizer with a scheduler that halfs learning rate after 200,000 steps\n",
    "optim_edsr = Adam(learning_rate=PiecewiseConstantDecay(boundaries=[20], values=[1e-4, 5e-5]))\n",
    "\n",
    "# Compile and train model for 300,000 steps with L1 pixel loss\n",
    "model_edsr.compile(optimizer=optim_edsr, loss='mean_absolute_error')\n",
    "model_edsr.fit(train_ds, epochs=3, steps_per_epoch=10)\n",
    "\n",
    "# Save model weights\n",
    "model_edsr.save_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "10/10 [==============================] - 3s 296ms/step - loss: 203.6635\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 3s 293ms/step - loss: 55.7923\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 3s 308ms/step - loss: 37.3619\n"
     ]
    }
   ],
   "source": [
    "from model.wdsr import wdsr_b\n",
    "\n",
    "# Custom WDSR B model (0.62M parameters)\n",
    "model_wdsr = wdsr_b(scale=4, num_res_blocks=32)\n",
    "\n",
    "# Adam optimizer with a scheduler that halfs learning rate after 200,000 steps\n",
    "optim_wdsr = Adam(learning_rate=PiecewiseConstantDecay(boundaries=[20], values=[1e-3, 5e-4]))\n",
    "\n",
    "# Compile and train model for 300,000 steps with L1 pixel loss\n",
    "model_wdsr.compile(optimizer=optim_wdsr, loss='mean_absolute_error')\n",
    "model_wdsr.fit(train_ds, epochs=3, steps_per_epoch=10)\n",
    "\n",
    "# Save weights\n",
    "model_wdsr.save_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "4.605159282684326\n",
      "1.1908142566680908\n",
      "1.182347059249878\n",
      "1.1828358173370361\n",
      "1.1803486347198486\n",
      "1.1828358173370361\n",
      "1.1938059329986572\n",
      "1.1798436641693115\n",
      "1.192317008972168\n",
      "1.2656135559082031\n",
      "10/20, perceptual loss = 0.0142, discriminator loss = 1.1983\n",
      "1.2596309185028076\n",
      "1.311490774154663\n",
      "1.2995233535766602\n",
      "1.1798439025878906\n",
      "1.1798431873321533\n",
      "1.1858279705047607\n",
      "1.1973059177398682\n",
      "1.1908140182495117\n",
      "1.195801019668579\n",
      "1.1948034763336182\n",
      "20/20, perceptual loss = 0.0146, discriminator loss = 0.7204\n"
     ]
    }
   ],
   "source": [
    "from model import srgan\n",
    "import time\n",
    "\n",
    "# Used in content_loss\n",
    "mean_squared_error = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Used in generator_loss and discriminator_loss\n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Model that computes the feature map after the 4th convolution \n",
    "# before the 5th max-pooling layer in VGG19. This is layer 20 in\n",
    "# the corresponding Keras model.\n",
    "mobileNet = srgan.mobileNet_54()\n",
    "\n",
    "# EDSR model used as generator in SRGAN\n",
    "generator = edsr(scale=4, num_res_blocks=16)\n",
    "generator.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))\n",
    "\n",
    "# SRGAN discriminator\n",
    "discriminator = srgan.discriminator()\n",
    "\n",
    "# Optmizers for generator and discriminator. SRGAN will be trained for\n",
    "# 200,000 steps and learning rate is reduced from 1e-4 to 1e-5 after\n",
    "# 100,000 steps\n",
    "schedule = PiecewiseConstantDecay(boundaries=[100], values=[1e-4, 1e-5])\n",
    "generator_optimizer = Adam(learning_rate=schedule)\n",
    "discriminator_optimizer = Adam(learning_rate=schedule)\n",
    "\n",
    "def generator_loss(sr_out):\n",
    "    return binary_cross_entropy(tf.ones_like(sr_out), sr_out)\n",
    "\n",
    "def discriminator_loss(hr_out, sr_out):\n",
    "    hr_loss = binary_cross_entropy(tf.ones_like(hr_out), hr_out)\n",
    "    sr_loss = binary_cross_entropy(tf.zeros_like(sr_out), sr_out)\n",
    "    return hr_loss + sr_loss\n",
    "\n",
    "@tf.function\n",
    "def content_loss(hr, sr):\n",
    "    sr = tf.keras.applications.mobilenet_v2.preprocess_input(sr)\n",
    "    hr = tf.keras.applications.mobilenet_v2.preprocess_input(hr)\n",
    "    sr_features = mobileNet(sr) / 12.75\n",
    "    hr_features = mobileNet(hr) / 12.75\n",
    "    return mean_squared_error(hr_features, sr_features)\n",
    "\n",
    "@tf.function\n",
    "def train_step(lr, hr):\n",
    "    \"\"\"SRGAN training step.\n",
    "    \n",
    "    Takes an LR and an HR image batch as input and returns\n",
    "    the computed perceptual loss and discriminator loss.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        lr = tf.cast(lr, tf.float32)\n",
    "        hr = tf.cast(hr, tf.float32)\n",
    "\n",
    "        # Forward pass\n",
    "        sr = generator(lr, training=True)\n",
    "        hr_output = discriminator(hr, training=True)\n",
    "        sr_output = discriminator(sr, training=True)\n",
    "\n",
    "        # Compute losses\n",
    "        con_loss = content_loss(hr, sr)\n",
    "        gen_loss = generator_loss(sr_output)\n",
    "        perc_loss = con_loss + 0.001 * gen_loss\n",
    "        disc_loss = discriminator_loss(hr_output, sr_output)\n",
    "\n",
    "    # Compute gradient of perceptual loss w.r.t. generator weights \n",
    "    gradients_of_generator = gen_tape.gradient(perc_loss, generator.trainable_variables)\n",
    "    # Compute gradient of discriminator loss w.r.t. discriminator weights \n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Update weights of generator and discriminator\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return perc_loss, disc_loss\n",
    "\n",
    "pls_metric = tf.keras.metrics.Mean()\n",
    "dls_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "steps = 20\n",
    "step = 0\n",
    "\n",
    "# Train SRGAN for 200,000 steps.\n",
    "for lr, hr in train_ds.take(steps):\n",
    "    start = time.time()\n",
    "    step += 1\n",
    "\n",
    "    pl, dl = train_step(lr, hr)\n",
    "    pls_metric(pl)\n",
    "    dls_metric(dl)\n",
    "\n",
    "    stop = time.time()\n",
    "    duration = stop-start\n",
    "    print(duration)\n",
    "    if step % 10 == 0:\n",
    "        print(f'{step}/{steps}, perceptual loss = {pls_metric.result():.4f}, discriminator loss = {dls_metric.result():.4f}')\n",
    "        pls_metric.reset_states()\n",
    "        dls_metric.reset_states()\n",
    "        \n",
    "generator.save_weights(os.path.join(weights_dir, 'weights-edsr-16-x4-fine-tuned.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WDSR B model used as generator in SRGAN\n",
    "generator = wdsr_b(scale=4, num_res_blocks=32)\n",
    "generator.load_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4.h5'))\n",
    "# Run SRGAN training ...\n",
    "generator.save_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4-fine-tuned.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import resolve_single\n",
    "from utils import load_image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def resolve_and_plot(model_pre_trained, model_fine_tuned, lr_image_path):\n",
    "    lr = load_image(lr_image_path)\n",
    "    \n",
    "    sr_pt = resolve_single(model_pre_trained, lr)\n",
    "    sr_ft = resolve_single(model_fine_tuned, lr)\n",
    "    \n",
    "    plt.figure(figsize=(20, 20))\n",
    "    \n",
    "    model_name = model_pre_trained.name.upper()\n",
    "    images = [lr, sr_pt, sr_ft]\n",
    "    titles = ['LR', f'SR ({model_name}, pixel loss)', f'SR ({model_name}, perceptual loss)']\n",
    "    positions = [1, 3, 4]\n",
    "    \n",
    "    for i, (image, title, position) in enumerate(zip(images, titles, positions)):\n",
    "        plt.subplot(2, 2, position)\n",
    "        plt.imshow(image)\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "weights_dir = 'weights/article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edsr_pre_trained = edsr(scale=4, num_res_blocks=16)\n",
    "edsr_pre_trained.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4.h5'))\n",
    "\n",
    "edsr_fine_tuned = edsr(scale=4, num_res_blocks=16)\n",
    "edsr_fine_tuned.load_weights(os.path.join(weights_dir, 'weights-edsr-16-x4-fine-tuned.h5'))\n",
    "\n",
    "resolve_and_plot(edsr_pre_trained, edsr_fine_tuned, 'demo/0001x4-crop.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.wdsr import wdsr_b\n",
    "\n",
    "wdsr_pre_trained = wdsr_b(scale=4, num_res_blocks=32)\n",
    "wdsr_pre_trained.load_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4.h5'))\n",
    "\n",
    "wdsr_fine_tuned = wdsr_b(scale=4, num_res_blocks=32)\n",
    "wdsr_fine_tuned.load_weights(os.path.join(weights_dir, 'weights-wdsr-b-32-x4-fine-tuned.h5'))\n",
    "\n",
    "resolve_and_plot(wdsr_pre_trained, wdsr_fine_tuned, 'demo/0002x4-crop.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sisr",
   "language": "python",
   "name": "sisr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
